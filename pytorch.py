# -*- coding: utf-8 -*-
"""pytorch fundamentals.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/13uRbLTugvR_KIAKuwDue317rIH6Zqfl_
"""
from time import time
from pandas._typing import ListLike
import torch
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

# Scalar
scalar = torch.tensor(7)
scalar

scalar.ndim

# Get the Python number within a tensor (only works with one-element tensors)
scalar.item()

# Vector
vector = torch.tensor([7, 7])
vector

# Check the number of dimensions of vector
vector.ndim

# Check shape of vector
vector.shape

# Matrix
MATRIX = torch.tensor([[7, 8],
                       [9, 10]])
MATRIX

MATRIX.shape

# Check number of dimensions
MATRIX.ndim

import torch
# Tensor
TENSOR = torch.tensor([[[1, 2, 3],
                        [3, 6, 9],
                        [2, 4, 5]]])
TENSOR

# Check number of dimensions for TENSOR
TENSOR.ndim

# Check shape of TENSOR
TENSOR.shape

newTen = torch.tensor([[1,2]])
newTen
newTen.ndim
newTen.shape

"""
## Random tensors"""

# Create a random tensor of size (3, 4)
random_tensor = torch.rand(size=(1, 3, 4))
print(random_tensor)
print(random_tensor, random_tensor.dtype)



# Zeros and Ones
zeros = torch.zeros(size=(3,4))
print(zeros)


ones = torch.ones(size=(3,4))
print(ones)




# Creating a range of tensors and tensors-Like
print(torch.arange(0,10))
print(torch.arange(start=1, end=11, step=2))



# ten_zeros = torch.zeros_like(input=one_to_ten)
# print(ten_zeros)


# Tensor Data types
float_32 = torch.tensor([3.0,6.0,9.0], dtype=None, device="cpu", requires_grad=False)
print(float_32)

float_16 = float_32.type(torch.float16)
print(float_16)

print(float_16 * float_32)

int_32 = torch.tensor([3,6,9], dtype=torch.int32)
print(int_32)
print(float_32 * int_32)



# Getting information from tensors
# datatype = tensor.dtype
# shape = tensor.shape
# device = tensor.device

# Create a tensor
some_tensor = torch.rand([3,4])
print(some_tensor)
print(f"Datatype of tensor: {some_tensor.dtype}")
print(f"Device: {some_tensor.device}")
print(f"Shape of Tensor: {some_tensor.shape}")



# Manipulating tensors
# Addition
tensor = torch.tensor([1,2,3])
print(tensor + 10)
print(tensor)


# Multiplcation
tensor = torch.tensor([4,5,6])
print(tensor * 10)
print(tensor)

# subtraction
tensor = torch.tensor([1,2,3])
print(tensor - 10)
print(tensor)


# division
tensor = torch.tensor([1,2,3])
print(tensor / 10)
print(tensor)

# inbuilt functions
print(torch.add(tensor, 10))
print(torch.mul(tensor, 10))
print(torch.sub(tensor, 10))



# Matrix multiplication
# element wise multiplication
print(torch.mul(tensor, tensor))

# matrix multiplication
print(torch.matmul(tensor, tensor))

# for loop
value = 0
for i in range(len(tensor)):
    value += tensor[i] * tensor[i]
print(value)

# Example 1: (3, 2) @ (2, 3) -> (3, 3)
tensor_A = torch.randn(3, 2)
tensor_B = torch.randn(2, 3)
print(f"Resulting shape: {torch.matmul(tensor_A, tensor_B).shape}")

# Example 2: (10, 5) @ (5, 2) -> (10, 2)
tensor_C = torch.randn(10, 5)
tensor_D = torch.randn(5, 2)
print(f"Resulting shape: {torch.matmul(tensor_C, tensor_D).shape}")

# Shapes for matrix multiplication
tensor_A = torch.tensor([[1,2],
             [3,4],
             [5,6]])
tensor_B = torch.tensor([[7,8],
             [9,10],
             [11,12]])
print(tensor_A.shape)
print(tensor_B.shape)
# we cannot multiply because of shape mismatch
# so we need to tranpose tensor_B
# print(torch.matmul(tensor_A, tensor_B))



# Transpose
tensor_B.T
print(tensor_B.T.shape)
print(tensor_B.T)
print(torch.matmul(tensor_A, tensor_B.T))

# The matrix multiplication works when B is transposed
# their shapes are (3,2) and (2,3) respectively
# and the resulting shape is (3,3)
print(f"Shape of tensor_A: {tensor_A.shape}")
print(f"Shape of tensor_B: {tensor_B.shape}")
print(f"Shape of tensor_B.T: {tensor_B.T.shape}")
print(f"Shape of resulting tensor: {torch.matmul(tensor_A, tensor_B.T).shape}")




# Tensor Aggregation

# create a tensor
x = torch.arange(1, 100, 10)
print(x)

# find the minimum value
print(torch.min(x))

# find the maximum value
print(torch.max(x))

# find the mean value
print(torch.mean(x.type(torch.float32)))
print(x.type(torch.float32).mean())

# find the sum value
print(torch.sum(x))

# find the index of the minimum value
print(torch.argmin(x))

# find the index of the maximum value
print(torch.argmax(x))



# Tensor Reshaping, stacking, squeezing, unsqueezing, permute
# define all the above terms
# Reshaping: Changing the shape of a tensor without changing its data
# Stacking: Combining multiple tensors into a single tensor
# Squeezing: Removing dimensions of size 1 from a tensor
# Unsqueezing: Adding dimensions of size 1 to a tensor
# Permute: Rearranging the dimensions of a tensor

# Create a tensor
x = torch.arange(1, 100, 10)
print(x)
print(x.shape)
# Reshape the tensor
x = x.reshape(10, 1)
print(x)

x = x.reshape(1, 10)
print(x)

# Reshape the tensor
x = x.reshape(10, 1)
print(x)

# Reshape the tensor
x = x.reshape(5, 2)
print(x)

# Reshape the tensor
x = x.reshape(2, 5)
print(x)


# change the shape of the tensor
x = x.view(1, 10)
print(x)

# change the shape of the tensor
x = x.view(10, 1)
print(x)


#  Stacking
# Stacking combines multiple tensors along a new dimension

# Create multiple tensors
x = torch.arange(1, 11)
y = torch.arange(11, 21)
z = torch.arange(21, 31)

print("Original tensors:")
print(f"x: {x}")
print(f"y: {y}")
print(f"z: {z}")

# Stack tensors along dimension 0 (creates new dimension)
stacked = torch.stack([x, y, z], dim=0)
print(f"\nStacked along dim=0:\n{stacked}")
print(f"Shape: {stacked.shape}")

# Stack tensors along dimension 1
stacked_dim1 = torch.stack([x, y, z], dim=1)
print(f"\nStacked along dim=1:\n{stacked_dim1}")
print(f"Shape: {stacked_dim1.shape}")

# vstack (vertical stack) - stacks tensors in sequence vertically (row-wise)
vstacked = torch.vstack([x, y, z])
print(f"\nvstack:\n{vstacked}")
print(f"Shape: {vstacked.shape}")

# hstack (horizontal stack) - stacks tensors in sequence horizontally (column-wise)
hstacked = torch.hstack([x, y, z])
print(f"\nhstack:\n{hstacked}")
print(f"Shape: {hstacked.shape}")


# Squeezing

# Create a tensor with a dimension of size 1
x = torch.zeros(2, 1, 2, 1, 2)
print(f"Original tensor shape: {x.shape}")

# Squeeze the tensor (removes all dimensions of size 1)
x_squeezed = x.squeeze()
print(f"Squeezed tensor shape: {x_squeezed.shape}")

# Squeeze a specific dimension
x_squeezed_dim1 = x.squeeze(dim=1)
print(f"Squeezed at dim 1 shape: {x_squeezed_dim1.shape}")


# Unsqueeze
# Unsqueeze adds a dimension of size 1 at a specified position

# Create a 1D tensor
y = torch.arange(1, 11)
print(f"\nOriginal tensor: {y}")
print(f"Original shape: {y.shape}")

# Unsqueeze at dimension 0 (adds dimension at the beginning)
y_unsqueezed_dim0 = y.unsqueeze(dim=0)
print(f"\nUnsqueezed at dim 0: {y_unsqueezed_dim0}")
print(f"Shape: {y_unsqueezed_dim0.shape}")

# Unsqueeze at dimension 1 (adds dimension at the end)
y_unsqueezed_dim1 = y.unsqueeze(dim=1)
print(f"\nUnsqueezed at dim 1:\n{y_unsqueezed_dim1}")
print(f"Shape: {y_unsqueezed_dim1.shape}")

# Create a 2D tensor and unsqueeze
z = torch.arange(1, 7).reshape(2, 3)
print(f"\n2D tensor:\n{z}")
print(f"Shape: {z.shape}")

# Unsqueeze at different dimensions
z_unsqueezed_dim0 = z.unsqueeze(dim=0)
print(f"\nUnsqueezed at dim 0 shape: {z_unsqueezed_dim0.shape}")

z_unsqueezed_dim1 = z.unsqueeze(dim=1)
print(f"Unsqueezed at dim 1 shape: {z_unsqueezed_dim1.shape}")

z_unsqueezed_dim2 = z.unsqueeze(dim=2)
print(f"Unsqueezed at dim 2 shape: {z_unsqueezed_dim2.shape}")


# Permute
# Permute rearranges the dimensions of a tensor

# Create a 3D tensor (batch_size, height, width)
image_tensor = torch.randn(64, 92, 32)
print(f"Original tensor shape: {image_tensor.shape}")

# Permute dimensions (swap height and width)
permuted_tensor = image_tensor.permute(0, 2, 1)
print(f"Permuted tensor shape: {permuted_tensor.shape}")

# Permute to (height, width, batch_size)
permuted_tensor2 = image_tensor.permute(1, 2, 0)
print(f"Permuted tensor shape: {permuted_tensor2.shape}")

# Permute with negative indices
permuted_tensor3 = image_tensor.permute(-1, 0, -2)
print(f"Permuted tensor shape: {permuted_tensor3.shape}")

# Example: Transpose (special case of permute for 2D tensors)
two_d_tensor = torch.randn(3, 4)
print(f"\n2D tensor shape: {two_d_tensor.shape}")
transposed_tensor = two_d_tensor.transpose(0, 1)
print(f"Transposed tensor shape: {transposed_tensor.shape}")


# Indexing and selecting data in PyTorch

# Create a tensor
x = torch.arange(1, 10).reshape(3, 3)
print(x)

# Indexing
print(x[1, 1])

# Slicing
print(x[1, :])

# Advanced indexing
print(x[[0, 2], [0, 2]])

# Boolean indexing
print(x[x > 5])
